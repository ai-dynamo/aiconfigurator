HuggingFaceID,Architecture,System,Backend,Version,Mode,Status,ErrMsg
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 67, in agg_pareto\n    model = get_model(\n            ^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 111, in get_model\n    model = LLAMAModel(\n            ^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 482, in __init__\n    super().__init__(*args)\n  File ""src/aiconfigurator/sdk/models.py"", line 315, in __init__\n    assert self._num_heads % model_config.tp_size == 0, (\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: num_heads 12 should be divisible by tp_size 8 \n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: num_heads 12 should be divisible by tp_size 8 \n"
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 486, in _get_summary_df\n    model = models.get_model(\n            ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 111, in get_model\n    model = LLAMAModel(\n            ^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 482, in __init__\n    super().__init__(*args)\n  File ""src/aiconfigurator/sdk/models.py"", line 315, in __init__\n    assert self._num_heads % model_config.tp_size == 0, (\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: num_heads 12 should be divisible by tp_size 8 \n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: num_heads 12 should be divisible by tp_size 8 \n"
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,a100_sxm,trtllm,1.0.0,agg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,a100_sxm,trtllm,1.0.0,disagg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,a100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,a100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,l40s,sglang,0.5.5.post3,agg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,l40s,sglang,0.5.5.post3,disagg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,l40s,trtllm,1.0.0,agg,PASS,
Qwen/Qwen2.5-1.5B,Qwen2ForCausalLM,l40s,trtllm,1.0.0,disagg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,a100_sxm,trtllm,1.0.0,agg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,a100_sxm,trtllm,1.0.0,disagg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,a100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,a100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,l40s,sglang,0.5.5.post3,agg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,l40s,sglang,0.5.5.post3,disagg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,l40s,trtllm,1.0.0,agg,PASS,
Qwen/Qwen2.5-32B,Qwen2ForCausalLM,l40s,trtllm,1.0.0,disagg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,a100_sxm,trtllm,1.0.0,agg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,a100_sxm,trtllm,1.0.0,disagg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,a100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,a100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,l40s,sglang,0.5.5.post3,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 143, in agg_pareto\n    f""No results found for any parallel configuration. Showing last exception: {exceptions[-1]}""\n                                                                                ~~~~~~~~~~^^^^\nIndexError: list index out of range\n"
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,l40s,sglang,0.5.5.post3,disagg,FAIL,"Configuration returned no results, failed to catch traceback"
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,l40s,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 143, in agg_pareto\n    f""No results found for any parallel configuration. Showing last exception: {exceptions[-1]}""\n                                                                                ~~~~~~~~~~^^^^\nIndexError: list index out of range\n"
Qwen/Qwen2.5-72B,Qwen2ForCausalLM,l40s,trtllm,1.0.0,disagg,FAIL,"Configuration returned no results, failed to catch traceback"
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 67, in agg_pareto\n    model = get_model(\n            ^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 111, in get_model\n    model = LLAMAModel(\n            ^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 482, in __init__\n    super().__init__(*args)\n  File ""src/aiconfigurator/sdk/models.py"", line 315, in __init__\n    assert self._num_heads % model_config.tp_size == 0, (\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: num_heads 28 should be divisible by tp_size 8 \n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: num_heads 28 should be divisible by tp_size 8 \n"
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 486, in _get_summary_df\n    model = models.get_model(\n            ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 111, in get_model\n    model = LLAMAModel(\n            ^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 482, in __init__\n    super().__init__(*args)\n  File ""src/aiconfigurator/sdk/models.py"", line 315, in __init__\n    assert self._num_heads % model_config.tp_size == 0, (\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: num_heads 28 should be divisible by tp_size 8 \n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: num_heads 28 should be divisible by tp_size 8 \n"
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,a100_sxm,trtllm,1.0.0,agg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,a100_sxm,trtllm,1.0.0,disagg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,a100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,a100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,l40s,sglang,0.5.5.post3,agg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,l40s,sglang,0.5.5.post3,disagg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,l40s,trtllm,1.0.0,agg,PASS,
Qwen/Qwen2.5-7B,Qwen2ForCausalLM,l40s,trtllm,1.0.0,disagg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,a100_sxm,trtllm,1.0.0,agg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,a100_sxm,trtllm,1.0.0,disagg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,a100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,a100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,l40s,sglang,0.5.5.post3,agg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,l40s,sglang,0.5.5.post3,disagg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,l40s,trtllm,1.0.0,agg,PASS,
Qwen/Qwen3-0.6B,Qwen3ForCausalLM,l40s,trtllm,1.0.0,disagg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,a100_sxm,trtllm,1.0.0,agg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,a100_sxm,trtllm,1.0.0,disagg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,a100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,a100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,l40s,sglang,0.5.5.post3,agg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,l40s,sglang,0.5.5.post3,disagg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,l40s,trtllm,1.0.0,agg,PASS,
Qwen/Qwen3-1.7B,Qwen3ForCausalLM,l40s,trtllm,1.0.0,disagg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,a100_sxm,trtllm,1.0.0,agg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,a100_sxm,trtllm,1.0.0,disagg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,a100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,a100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,l40s,sglang,0.5.5.post3,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 67, in agg_pareto\n    model = get_model(\n            ^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 127, in get_model\n    model = MOEModel(\n            ^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 655, in __init__\n    self._validate_fp8_block_quantized_moe_config()\n  File ""src/aiconfigurator/sdk/models.py"", line 923, in _validate_fp8_block_quantized_moe_config\n    raise ValueError(\nValueError: Invalid quantized MoE configuration: (moe_intermediate_size=1536 / moe_tp_size=8) % weight_block_size=128 != 0. \n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Invalid quantized MoE configuration: (moe_intermediate_size=1536 / moe_tp_size=8) % weight_block_size=128 != 0. \n"
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,l40s,sglang,0.5.5.post3,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 486, in _get_summary_df\n    model = models.get_model(\n            ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 127, in get_model\n    model = MOEModel(\n            ^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 655, in __init__\n    self._validate_fp8_block_quantized_moe_config()\n  File ""src/aiconfigurator/sdk/models.py"", line 923, in _validate_fp8_block_quantized_moe_config\n    raise ValueError(\nValueError: Invalid quantized MoE configuration: (moe_intermediate_size=1536 / moe_tp_size=8) % weight_block_size=128 != 0. \n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Invalid quantized MoE configuration: (moe_intermediate_size=1536 / moe_tp_size=8) % weight_block_size=128 != 0. \n"
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,l40s,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
Qwen/Qwen3-235B-A22B,Qwen3MoeForCausalLM,l40s,trtllm,1.0.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,a100_sxm,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,a100_sxm,trtllm,1.0.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,a100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,a100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,gb200,trtllm,1.0.0rc6,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 306, in query\n    comm_latency = database.query_custom_allreduce(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3420, in query_custom_allreduce\n    size_left, size_right = self._nearest_1d_point_helper(size, list(comm_dict.keys()), inner_only=False)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,gb200,trtllm,1.0.0rc6,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 306, in query\n    comm_latency = database.query_custom_allreduce(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3420, in query_custom_allreduce\n    size_left, size_right = self._nearest_1d_point_helper(size, list(comm_dict.keys()), inner_only=False)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 67, in agg_pareto\n    model = get_model(\n            ^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 127, in get_model\n    model = MOEModel(\n            ^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 655, in __init__\n    self._validate_fp8_block_quantized_moe_config()\n  File ""src/aiconfigurator/sdk/models.py"", line 923, in _validate_fp8_block_quantized_moe_config\n    raise ValueError(\nValueError: Invalid quantized MoE configuration: (moe_intermediate_size=768 / moe_tp_size=8) % weight_block_size=128 != 0. \n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Invalid quantized MoE configuration: (moe_intermediate_size=768 / moe_tp_size=8) % weight_block_size=128 != 0. \n"
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 486, in _get_summary_df\n    model = models.get_model(\n            ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 127, in get_model\n    model = MOEModel(\n            ^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 655, in __init__\n    self._validate_fp8_block_quantized_moe_config()\n  File ""src/aiconfigurator/sdk/models.py"", line 923, in _validate_fp8_block_quantized_moe_config\n    raise ValueError(\nValueError: Invalid quantized MoE configuration: (moe_intermediate_size=768 / moe_tp_size=8) % weight_block_size=128 != 0. \n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Invalid quantized MoE configuration: (moe_intermediate_size=768 / moe_tp_size=8) % weight_block_size=128 != 0. \n"
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,l40s,sglang,0.5.5.post3,agg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,l40s,sglang,0.5.5.post3,disagg,PASS,
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,l40s,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
Qwen/Qwen3-30B-A3B,Qwen3MoeForCausalLM,l40s,trtllm,1.0.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
Qwen/Qwen3-32B,Qwen3ForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
Qwen/Qwen3-32B,Qwen3ForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
Qwen/Qwen3-32B,Qwen3ForCausalLM,a100_sxm,trtllm,1.0.0,agg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,a100_sxm,trtllm,1.0.0,disagg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,a100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,a100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,l40s,sglang,0.5.5.post3,agg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,l40s,sglang,0.5.5.post3,disagg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,l40s,trtllm,1.0.0,agg,PASS,
Qwen/Qwen3-32B,Qwen3ForCausalLM,l40s,trtllm,1.0.0,disagg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
Qwen/Qwen3-8B,Qwen3ForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
Qwen/Qwen3-8B,Qwen3ForCausalLM,a100_sxm,trtllm,1.0.0,agg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,a100_sxm,trtllm,1.0.0,disagg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,a100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,a100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,l40s,sglang,0.5.5.post3,agg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,l40s,sglang,0.5.5.post3,disagg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,l40s,trtllm,1.0.0,agg,PASS,
Qwen/Qwen3-8B,Qwen3ForCausalLM,l40s,trtllm,1.0.0,disagg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,a100_sxm,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,a100_sxm,trtllm,1.0.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,a100_sxm,vllm,0.12.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 409, in query\n    assert self._moe_tp_size == 1 or self._moe_ep_size == 1, (\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: vllm does not support MoE TP and MoE EP at the same time\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: vllm does not support MoE TP and MoE EP at the same time\n"
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,a100_sxm,vllm,0.12.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 409, in query\n    assert self._moe_tp_size == 1 or self._moe_ep_size == 1, (\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: vllm does not support MoE TP and MoE EP at the same time\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: vllm does not support MoE TP and MoE EP at the same time\n"
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 67, in agg_pareto\n    model = get_model(\n            ^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 127, in get_model\n    model = MOEModel(\n            ^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 655, in __init__\n    self._validate_fp8_block_quantized_moe_config()\n  File ""src/aiconfigurator/sdk/models.py"", line 923, in _validate_fp8_block_quantized_moe_config\n    raise ValueError(\nValueError: Invalid quantized MoE configuration: (moe_intermediate_size=2560 / moe_tp_size=8) % weight_block_size=128 != 0. \n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Invalid quantized MoE configuration: (moe_intermediate_size=2560 / moe_tp_size=8) % weight_block_size=128 != 0. \n"
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 486, in _get_summary_df\n    model = models.get_model(\n            ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 127, in get_model\n    model = MOEModel(\n            ^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 655, in __init__\n    self._validate_fp8_block_quantized_moe_config()\n  File ""src/aiconfigurator/sdk/models.py"", line 923, in _validate_fp8_block_quantized_moe_config\n    raise ValueError(\nValueError: Invalid quantized MoE configuration: (moe_intermediate_size=2560 / moe_tp_size=8) % weight_block_size=128 != 0. \n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Invalid quantized MoE configuration: (moe_intermediate_size=2560 / moe_tp_size=8) % weight_block_size=128 != 0. \n"
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,l40s,sglang,0.5.5.post3,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 67, in agg_pareto\n    model = get_model(\n            ^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 127, in get_model\n    model = MOEModel(\n            ^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 655, in __init__\n    self._validate_fp8_block_quantized_moe_config()\n  File ""src/aiconfigurator/sdk/models.py"", line 923, in _validate_fp8_block_quantized_moe_config\n    raise ValueError(\nValueError: Invalid quantized MoE configuration: (moe_intermediate_size=2560 / moe_tp_size=8) % weight_block_size=128 != 0. \n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Invalid quantized MoE configuration: (moe_intermediate_size=2560 / moe_tp_size=8) % weight_block_size=128 != 0. \n"
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,l40s,sglang,0.5.5.post3,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 486, in _get_summary_df\n    model = models.get_model(\n            ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 127, in get_model\n    model = MOEModel(\n            ^^^^^^^^^\n  File ""src/aiconfigurator/sdk/models.py"", line 655, in __init__\n    self._validate_fp8_block_quantized_moe_config()\n  File ""src/aiconfigurator/sdk/models.py"", line 923, in _validate_fp8_block_quantized_moe_config\n    raise ValueError(\nValueError: Invalid quantized MoE configuration: (moe_intermediate_size=2560 / moe_tp_size=8) % weight_block_size=128 != 0. \n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Invalid quantized MoE configuration: (moe_intermediate_size=2560 / moe_tp_size=8) % weight_block_size=128 != 0. \n"
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,l40s,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
Qwen/Qwen3-Coder-480B-A35B-Instruct,Qwen3MoeForCausalLM,l40s,trtllm,1.0.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 659, in query\n    result = database.query_context_mla(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2936, in query_context_mla\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context MLA perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context MLA perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 659, in query\n    result = database.query_context_mla(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2936, in query_context_mla\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context MLA perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context MLA perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,a100_sxm,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,a100_sxm,trtllm,1.0.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,a100_sxm,vllm,0.12.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 874, in validate\n    raise NotImplementedError(""AIConfigurator does not yet support DEEPSEEK models for VLLM backend."")\nNotImplementedError: AIConfigurator does not yet support DEEPSEEK models for VLLM backend.\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,a100_sxm,vllm,0.12.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 874, in validate\n    raise NotImplementedError(""AIConfigurator does not yet support DEEPSEEK models for VLLM backend."")\nNotImplementedError: AIConfigurator does not yet support DEEPSEEK models for VLLM backend.\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 448, in query\n    assert self._attention_tp_size == 1 or self._attention_dp_size == 1, (\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: We don't enable the path for non-wideep SGLang to support TP>1 and DP>1 for attn simultaneously\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: We don't enable the path for non-wideep SGLang to support TP>1 and DP>1 for attn simultaneously\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 448, in query\n    assert self._attention_tp_size == 1 or self._attention_dp_size == 1, (\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: We don't enable the path for non-wideep SGLang to support TP>1 and DP>1 for attn simultaneously\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: We don't enable the path for non-wideep SGLang to support TP>1 and DP>1 for attn simultaneously\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 143, in agg_pareto\n    f""No results found for any parallel configuration. Showing last exception: {exceptions[-1]}""\n                                                                                ~~~~~~~~~~^^^^\nIndexError: list index out of range\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 530, in _get_summary_df\n    f""No results found for any parallel configuration. Showing last exception: {exceptions[-1]}""\n                                                                                ~~~~~~~~~~^^^^\nIndexError: list index out of range\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 143, in agg_pareto\n    f""No results found for any parallel configuration. Showing last exception: {exceptions[-1]}""\n                                                                                ~~~~~~~~~~^^^^\nIndexError: list index out of range\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 530, in _get_summary_df\n    f""No results found for any parallel configuration. Showing last exception: {exceptions[-1]}""\n                                                                                ~~~~~~~~~~^^^^\nIndexError: list index out of range\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,h100_sxm,vllm,0.12.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 874, in validate\n    raise NotImplementedError(""AIConfigurator does not yet support DEEPSEEK models for VLLM backend."")\nNotImplementedError: AIConfigurator does not yet support DEEPSEEK models for VLLM backend.\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,h100_sxm,vllm,0.12.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 874, in validate\n    raise NotImplementedError(""AIConfigurator does not yet support DEEPSEEK models for VLLM backend."")\nNotImplementedError: AIConfigurator does not yet support DEEPSEEK models for VLLM backend.\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,h100_sxm,vllm,0.14.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 874, in validate\n    raise NotImplementedError(""AIConfigurator does not yet support DEEPSEEK models for VLLM backend."")\nNotImplementedError: AIConfigurator does not yet support DEEPSEEK models for VLLM backend.\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,h100_sxm,vllm,0.14.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 874, in validate\n    raise NotImplementedError(""AIConfigurator does not yet support DEEPSEEK models for VLLM backend."")\nNotImplementedError: AIConfigurator does not yet support DEEPSEEK models for VLLM backend.\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,h200_sxm,vllm,0.12.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 874, in validate\n    raise NotImplementedError(""AIConfigurator does not yet support DEEPSEEK models for VLLM backend."")\nNotImplementedError: AIConfigurator does not yet support DEEPSEEK models for VLLM backend.\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,h200_sxm,vllm,0.12.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 874, in validate\n    raise NotImplementedError(""AIConfigurator does not yet support DEEPSEEK models for VLLM backend."")\nNotImplementedError: AIConfigurator does not yet support DEEPSEEK models for VLLM backend.\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,l40s,sglang,0.5.5.post3,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 659, in query\n    result = database.query_context_mla(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2936, in query_context_mla\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context MLA perf table is missing for system='l40s', backend='sglang', version='0.5.5.post3'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context MLA perf table is missing for system='l40s', backend='sglang', version='0.5.5.post3'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,l40s,sglang,0.5.5.post3,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 659, in query\n    result = database.query_context_mla(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2936, in query_context_mla\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context MLA perf table is missing for system='l40s', backend='sglang', version='0.5.5.post3'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context MLA perf table is missing for system='l40s', backend='sglang', version='0.5.5.post3'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,l40s,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
deepseek-ai/DeepSeek-V3,DeepseekV3ForCausalLM,l40s,trtllm,1.0.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,a100_sxm,trtllm,1.0.0,agg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,a100_sxm,trtllm,1.0.0,disagg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,a100_sxm,vllm,0.12.0,agg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,a100_sxm,vllm,0.12.0,disagg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,l40s,sglang,0.5.5.post3,agg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,l40s,sglang,0.5.5.post3,disagg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,l40s,trtllm,1.0.0,agg,PASS,
meta-llama/Llama-2-13b-hf,LlamaForCausalLM,l40s,trtllm,1.0.0,disagg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,a100_sxm,trtllm,1.0.0,agg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,a100_sxm,trtllm,1.0.0,disagg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,a100_sxm,vllm,0.12.0,agg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,a100_sxm,vllm,0.12.0,disagg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,l40s,sglang,0.5.5.post3,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 143, in agg_pareto\n    f""No results found for any parallel configuration. Showing last exception: {exceptions[-1]}""\n                                                                                ~~~~~~~~~~^^^^\nIndexError: list index out of range\n"
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,l40s,sglang,0.5.5.post3,disagg,FAIL,"Configuration returned no results, failed to catch traceback"
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,l40s,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 143, in agg_pareto\n    f""No results found for any parallel configuration. Showing last exception: {exceptions[-1]}""\n                                                                                ~~~~~~~~~~^^^^\nIndexError: list index out of range\n"
meta-llama/Llama-2-70b-hf,LlamaForCausalLM,l40s,trtllm,1.0.0,disagg,FAIL,"Configuration returned no results, failed to catch traceback"
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,a100_sxm,trtllm,1.0.0,agg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,a100_sxm,trtllm,1.0.0,disagg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,a100_sxm,vllm,0.12.0,agg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,a100_sxm,vllm,0.12.0,disagg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,l40s,sglang,0.5.5.post3,agg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,l40s,sglang,0.5.5.post3,disagg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,l40s,trtllm,1.0.0,agg,PASS,
meta-llama/Llama-2-7b-hf,LlamaForCausalLM,l40s,trtllm,1.0.0,disagg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,a100_sxm,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 143, in agg_pareto\n    f""No results found for any parallel configuration. Showing last exception: {exceptions[-1]}""\n                                                                                ~~~~~~~~~~^^^^\nIndexError: list index out of range\n"
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,a100_sxm,trtllm,1.0.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 530, in _get_summary_df\n    f""No results found for any parallel configuration. Showing last exception: {exceptions[-1]}""\n                                                                                ~~~~~~~~~~^^^^\nIndexError: list index out of range\n"
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,a100_sxm,vllm,0.12.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 143, in agg_pareto\n    f""No results found for any parallel configuration. Showing last exception: {exceptions[-1]}""\n                                                                                ~~~~~~~~~~^^^^\nIndexError: list index out of range\n"
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,a100_sxm,vllm,0.12.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 530, in _get_summary_df\n    f""No results found for any parallel configuration. Showing last exception: {exceptions[-1]}""\n                                                                                ~~~~~~~~~~^^^^\nIndexError: list index out of range\n"
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,l40s,sglang,0.5.5.post3,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 143, in agg_pareto\n    f""No results found for any parallel configuration. Showing last exception: {exceptions[-1]}""\n                                                                                ~~~~~~~~~~^^^^\nIndexError: list index out of range\n"
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,l40s,sglang,0.5.5.post3,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 530, in _get_summary_df\n    f""No results found for any parallel configuration. Showing last exception: {exceptions[-1]}""\n                                                                                ~~~~~~~~~~^^^^\nIndexError: list index out of range\n"
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,l40s,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 143, in agg_pareto\n    f""No results found for any parallel configuration. Showing last exception: {exceptions[-1]}""\n                                                                                ~~~~~~~~~~^^^^\nIndexError: list index out of range\n"
meta-llama/Meta-Llama-3.1-405B,LlamaForCausalLM,l40s,trtllm,1.0.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 530, in _get_summary_df\n    f""No results found for any parallel configuration. Showing last exception: {exceptions[-1]}""\n                                                                                ~~~~~~~~~~^^^^\nIndexError: list index out of range\n"
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,a100_sxm,trtllm,1.0.0,agg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,a100_sxm,trtllm,1.0.0,disagg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,a100_sxm,vllm,0.12.0,agg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,a100_sxm,vllm,0.12.0,disagg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,l40s,sglang,0.5.5.post3,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 143, in agg_pareto\n    f""No results found for any parallel configuration. Showing last exception: {exceptions[-1]}""\n                                                                                ~~~~~~~~~~^^^^\nIndexError: list index out of range\n"
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,l40s,sglang,0.5.5.post3,disagg,FAIL,"Configuration returned no results, failed to catch traceback"
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,l40s,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 143, in agg_pareto\n    f""No results found for any parallel configuration. Showing last exception: {exceptions[-1]}""\n                                                                                ~~~~~~~~~~^^^^\nIndexError: list index out of range\n"
meta-llama/Meta-Llama-3.1-70B,LlamaForCausalLM,l40s,trtllm,1.0.0,disagg,FAIL,"Configuration returned no results, failed to catch traceback"
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,a100_sxm,trtllm,1.0.0,agg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,a100_sxm,trtllm,1.0.0,disagg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,a100_sxm,vllm,0.12.0,agg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,a100_sxm,vllm,0.12.0,disagg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,l40s,sglang,0.5.5.post3,agg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,l40s,sglang,0.5.5.post3,disagg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,l40s,trtllm,1.0.0,agg,PASS,
meta-llama/Meta-Llama-3.1-8B,LlamaForCausalLM,l40s,trtllm,1.0.0,disagg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,a100_sxm,trtllm,1.0.0,agg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,a100_sxm,trtllm,1.0.0,disagg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,a100_sxm,vllm,0.12.0,agg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,a100_sxm,vllm,0.12.0,disagg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,l40s,sglang,0.5.5.post3,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 448, in query\n    assert self._attention_tp_size == 1 or self._attention_dp_size == 1, (\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: We don't enable the path for non-wideep SGLang to support TP>1 and DP>1 for attn simultaneously\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: We don't enable the path for non-wideep SGLang to support TP>1 and DP>1 for attn simultaneously\n"
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,l40s,sglang,0.5.5.post3,disagg,FAIL,"Configuration returned no results, failed to catch traceback"
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,l40s,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
mistralai/Mixtral-8x22B-v0.1,MixtralForCausalLM,l40s,trtllm,1.0.0,disagg,FAIL,"Configuration returned no results, failed to catch traceback"
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,a100_sxm,trtllm,1.0.0,agg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,a100_sxm,trtllm,1.0.0,disagg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,a100_sxm,vllm,0.12.0,agg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,a100_sxm,vllm,0.12.0,disagg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,l40s,sglang,0.5.5.post3,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 448, in query\n    assert self._attention_tp_size == 1 or self._attention_dp_size == 1, (\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: We don't enable the path for non-wideep SGLang to support TP>1 and DP>1 for attn simultaneously\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: We don't enable the path for non-wideep SGLang to support TP>1 and DP>1 for attn simultaneously\n"
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,l40s,sglang,0.5.5.post3,disagg,FAIL,"Configuration returned no results, failed to catch traceback"
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,l40s,trtllm,1.0.0,agg,PASS,
mistralai/Mixtral-8x7B-v0.1,MixtralForCausalLM,l40s,trtllm,1.0.0,disagg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 570, in query\n    result = database.query_context_attention(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2679, in query_context_attention\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Context attention perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,a100_sxm,trtllm,1.0.0,agg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,a100_sxm,trtllm,1.0.0,disagg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,a100_sxm,vllm,0.12.0,agg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,a100_sxm,vllm,0.12.0,disagg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,l40s,sglang,0.5.5.post3,agg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,l40s,sglang,0.5.5.post3,disagg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,l40s,trtllm,1.0.0,agg,PASS,
nvidia/Llama-3_3-Nemotron-Super-49B-v1,DeciLMForCausalLM,l40s,trtllm,1.0.0,disagg,PASS,
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 57, in query\n    result = database.query_custom_allreduce(common.CommQuantMode.half, self._tp_size, size)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3411, in query_custom_allreduce\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Custom allreduce perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Custom allreduce perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 57, in query\n    result = database.query_custom_allreduce(common.CommQuantMode.half, self._tp_size, size)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3411, in query_custom_allreduce\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Custom allreduce perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Custom allreduce perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,a100_sxm,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,a100_sxm,trtllm,1.0.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,a100_sxm,vllm,0.12.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3848, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,a100_sxm,vllm,0.12.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3848, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3755, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3755, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,gb200,trtllm,1.0.0rc6,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 57, in query\n    result = database.query_custom_allreduce(common.CommQuantMode.half, self._tp_size, size)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3420, in query_custom_allreduce\n    size_left, size_right = self._nearest_1d_point_helper(size, list(comm_dict.keys()), inner_only=False)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,gb200,trtllm,1.0.0rc6,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 57, in query\n    result = database.query_custom_allreduce(common.CommQuantMode.half, self._tp_size, size)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3420, in query_custom_allreduce\n    size_left, size_right = self._nearest_1d_point_helper(size, list(comm_dict.keys()), inner_only=False)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3755, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3755, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,h100_sxm,vllm,0.12.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3848, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,h100_sxm,vllm,0.12.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3848, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,h100_sxm,vllm,0.14.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3848, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,h100_sxm,vllm,0.14.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3848, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3755, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3755, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,h200_sxm,vllm,0.12.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3848, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,h200_sxm,vllm,0.12.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3848, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,l40s,sglang,0.5.5.post3,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3755, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,l40s,sglang,0.5.5.post3,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3755, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,l40s,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16,NemotronHForCausalLM,l40s,trtllm,1.0.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 57, in query\n    result = database.query_custom_allreduce(common.CommQuantMode.half, self._tp_size, size)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3411, in query_custom_allreduce\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Custom allreduce perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Custom allreduce perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 57, in query\n    result = database.query_custom_allreduce(common.CommQuantMode.half, self._tp_size, size)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3411, in query_custom_allreduce\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Custom allreduce perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Custom allreduce perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,a100_sxm,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,a100_sxm,trtllm,1.0.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,a100_sxm,vllm,0.12.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3848, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,a100_sxm,vllm,0.12.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3848, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3755, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3755, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,gb200,trtllm,1.0.0rc6,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 57, in query\n    result = database.query_custom_allreduce(common.CommQuantMode.half, self._tp_size, size)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3420, in query_custom_allreduce\n    size_left, size_right = self._nearest_1d_point_helper(size, list(comm_dict.keys()), inner_only=False)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,gb200,trtllm,1.0.0rc6,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 57, in query\n    result = database.query_custom_allreduce(common.CommQuantMode.half, self._tp_size, size)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3420, in query_custom_allreduce\n    size_left, size_right = self._nearest_1d_point_helper(size, list(comm_dict.keys()), inner_only=False)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3755, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3755, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,h100_sxm,vllm,0.12.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3848, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,h100_sxm,vllm,0.12.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3848, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,h100_sxm,vllm,0.14.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3848, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,h100_sxm,vllm,0.14.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3848, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3755, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3755, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,h200_sxm,vllm,0.12.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/vllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3848, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,h200_sxm,vllm,0.12.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3848, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,l40s,sglang,0.5.5.post3,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3755, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,l40s,sglang,0.5.5.post3,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3755, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,l40s,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 47, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 413, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 187, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/trtllm_backend.py"", line 101, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/NVIDIA-Nemotron-3-Super-120B-NVFP4-FP8KV,NemotronHForCausalLM,l40s,trtllm,1.0.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 227, in query\n    result = database.query_moe(\n             ^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3818, in query_moe\n    num_left, num_right = self._nearest_1d_point_helper(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 2147, in _nearest_1d_point_helper\n    assert values is not None and len(values) >= 2, ""values is None or len(values) < 2""\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: values is None or len(values) < 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: values is None or len(values) < 2\n"
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 45, in run_agg\n    summary = self._agg_cache[isl][osl][b][ctx_tokens]\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 4000\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 106, in agg_pareto\n    summary = sess.find_best_agg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 99, in find_best_agg_result_under_constraints\n    return self._backend.find_best_agg_result_under_constraints(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 412, in find_best_agg_result_under_constraints\n    summary = self.run_agg(\n              ^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 186, in run_agg\n    mix_step_latency_ms, mix_step_energy_wms = _get_mix_step_latency(\n                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/sglang_backend.py"", line 99, in _get_mix_step_latency\n    summary = self.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 57, in query\n    result = database.query_custom_allreduce(common.CommQuantMode.half, self._tp_size, size)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3411, in query_custom_allreduce\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Custom allreduce perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1291, in run\n    result = self.run_agg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1113, in run_agg\n    result_df = pa.agg_pareto(\n                ^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 142, in agg_pareto\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Custom allreduce perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 507, in _get_summary_df\n    summary = sess.run_static(\n              ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 66, in run_static\n    return self._backend.run_static(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 153, in run_static\n    context_latency_dict, context_energy_wms_dict = _run_context(batch_size, isl, prefix)\n                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/backends/base_backend.py"", line 81, in _run_context\n    result = op.query(database, x=x, batch_size=batch_size, beam_width=1, s=isl, prefix=prefix)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/operations.py"", line 57, in query\n    result = database.query_custom_allreduce(common.CommQuantMode.half, self._tp_size, size)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/perf_database.py"", line 3411, in query_custom_allreduce\n    raise PerfDataNotAvailableError(\naiconfigurator.sdk.perf_database.PerfDataNotAvailableError: Custom allreduce perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 135, in run_single_test\n    result = runner.run(task_config)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1293, in run\n    result = self.run_disagg(task_config.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 1259, in run_disagg\n    result_df = pa.disagg_pareto(\n                ^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/pareto_analysis.py"", line 248, in disagg_pareto\n    summary = disagg_sess.find_best_disagg_result_under_constraints(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 667, in find_best_disagg_result_under_constraints\n    prefill_summary_df = _get_summary_df(\n                         ^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/inference_session.py"", line 529, in _get_summary_df\n    raise RuntimeError(\nRuntimeError: No results found for any parallel configuration. Showing last exception: Custom allreduce perf table is missing for system='a100_sxm', backend='sglang', version='0.5.8'. Please use HYBRID or EMPIRICAL database mode, or provide the data file.\n"
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,a100_sxm,trtllm,1.0.0,agg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,a100_sxm,trtllm,1.0.0,disagg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,a100_sxm,vllm,0.12.0,agg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,a100_sxm,vllm,0.12.0,disagg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,gb200,trtllm,1.0.0rc6,agg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,gb200,trtllm,1.0.0rc6,disagg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,gb200,trtllm,1.2.0rc5,agg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,gb200,trtllm,1.2.0rc5,disagg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,h100_sxm,vllm,0.12.0,agg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,h100_sxm,vllm,0.12.0,disagg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,h100_sxm,vllm,0.14.0,agg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,h100_sxm,vllm,0.14.0,disagg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,h200_sxm,vllm,0.12.0,agg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,h200_sxm,vllm,0.12.0,disagg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,l40s,sglang,0.5.5.post3,agg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,l40s,sglang,0.5.5.post3,disagg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,l40s,trtllm,1.0.0,agg,PASS,
nvidia/Nemotron-H-56B-Base-8K,NemotronHForCausalLM,l40s,trtllm,1.0.0,disagg,PASS,
openai/gpt-oss-120b,GptOssForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='a100_sxm', backend='sglang', version='0.5.8'. Supported moe modes: ['float16']\n"
openai/gpt-oss-120b,GptOssForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='a100_sxm', backend='sglang', version='0.5.8'. Supported moe modes: ['float16']\n"
openai/gpt-oss-120b,GptOssForCausalLM,a100_sxm,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='a100_sxm', backend='trtllm', version='1.0.0'. Supported moe modes: ['float16']\n"
openai/gpt-oss-120b,GptOssForCausalLM,a100_sxm,trtllm,1.0.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='a100_sxm', backend='trtllm', version='1.0.0'. Supported moe modes: ['float16']\n"
openai/gpt-oss-120b,GptOssForCausalLM,a100_sxm,vllm,0.12.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='a100_sxm', backend='vllm', version='0.12.0'. Supported moe modes: ['float16']\n"
openai/gpt-oss-120b,GptOssForCausalLM,a100_sxm,vllm,0.12.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='a100_sxm', backend='vllm', version='0.12.0'. Supported moe modes: ['float16']\n"
openai/gpt-oss-120b,GptOssForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='b200_sxm', backend='sglang', version='0.5.6.post2'. Supported moe modes: ['float16', 'fp8_block', 'nvfp4']\n"
openai/gpt-oss-120b,GptOssForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='b200_sxm', backend='sglang', version='0.5.6.post2'. Supported moe modes: ['float16', 'fp8_block', 'nvfp4']\n"
openai/gpt-oss-120b,GptOssForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='b200_sxm', backend='trtllm', version='1.0.0rc6'. Supported moe modes: ['float16', 'fp8', 'nvfp4']\n"
openai/gpt-oss-120b,GptOssForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='b200_sxm', backend='trtllm', version='1.0.0rc6'. Supported moe modes: ['float16', 'fp8', 'nvfp4']\n"
openai/gpt-oss-120b,GptOssForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='b200_sxm', backend='trtllm', version='1.2.0rc5'. Supported moe modes: ['float16', 'fp8', 'nvfp4']\n"
openai/gpt-oss-120b,GptOssForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='b200_sxm', backend='trtllm', version='1.2.0rc5'. Supported moe modes: ['float16', 'fp8', 'nvfp4']\n"
openai/gpt-oss-120b,GptOssForCausalLM,gb200,trtllm,1.0.0rc6,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='gb200', backend='trtllm', version='1.0.0rc6'. Supported moe modes: ['float16', 'fp8', 'nvfp4']\n"
openai/gpt-oss-120b,GptOssForCausalLM,gb200,trtllm,1.0.0rc6,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='gb200', backend='trtllm', version='1.0.0rc6'. Supported moe modes: ['float16', 'fp8', 'nvfp4']\n"
openai/gpt-oss-120b,GptOssForCausalLM,gb200,trtllm,1.2.0rc5,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='gb200', backend='trtllm', version='1.2.0rc5'. Supported moe modes: ['float16', 'fp8', 'nvfp4']\n"
openai/gpt-oss-120b,GptOssForCausalLM,gb200,trtllm,1.2.0rc5,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='gb200', backend='trtllm', version='1.2.0rc5'. Supported moe modes: ['float16', 'fp8', 'nvfp4']\n"
openai/gpt-oss-120b,GptOssForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h100_sxm', backend='sglang', version='0.5.6.post2'. Supported moe modes: ['float16', 'fp8_block']\n"
openai/gpt-oss-120b,GptOssForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h100_sxm', backend='sglang', version='0.5.6.post2'. Supported moe modes: ['float16', 'fp8_block']\n"
openai/gpt-oss-120b,GptOssForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h100_sxm', backend='trtllm', version='1.0.0rc3'. Supported moe modes: ['float16', 'fp8', 'fp8_block', 'w4afp8']\n"
openai/gpt-oss-120b,GptOssForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h100_sxm', backend='trtllm', version='1.0.0rc3'. Supported moe modes: ['float16', 'fp8', 'fp8_block', 'w4afp8']\n"
openai/gpt-oss-120b,GptOssForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
openai/gpt-oss-120b,GptOssForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
openai/gpt-oss-120b,GptOssForCausalLM,h100_sxm,vllm,0.12.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h100_sxm', backend='vllm', version='0.12.0'. Supported moe modes: ['float16', 'fp8', 'fp8_block']\n"
openai/gpt-oss-120b,GptOssForCausalLM,h100_sxm,vllm,0.12.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h100_sxm', backend='vllm', version='0.12.0'. Supported moe modes: ['float16', 'fp8', 'fp8_block']\n"
openai/gpt-oss-120b,GptOssForCausalLM,h100_sxm,vllm,0.14.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h100_sxm', backend='vllm', version='0.14.0'. Supported moe modes: ['float16', 'fp8', 'fp8_block']\n"
openai/gpt-oss-120b,GptOssForCausalLM,h100_sxm,vllm,0.14.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h100_sxm', backend='vllm', version='0.14.0'. Supported moe modes: ['float16', 'fp8', 'fp8_block']\n"
openai/gpt-oss-120b,GptOssForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h200_sxm', backend='sglang', version='0.5.6.post2'. Supported moe modes: ['float16', 'fp8_block']\n"
openai/gpt-oss-120b,GptOssForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h200_sxm', backend='sglang', version='0.5.6.post2'. Supported moe modes: ['float16', 'fp8_block']\n"
openai/gpt-oss-120b,GptOssForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h200_sxm', backend='trtllm', version='1.0.0rc3'. Supported moe modes: ['float16', 'fp8', 'fp8_block', 'w4afp8']\n"
openai/gpt-oss-120b,GptOssForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h200_sxm', backend='trtllm', version='1.0.0rc3'. Supported moe modes: ['float16', 'fp8', 'fp8_block', 'w4afp8']\n"
openai/gpt-oss-120b,GptOssForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
openai/gpt-oss-120b,GptOssForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
openai/gpt-oss-120b,GptOssForCausalLM,h200_sxm,vllm,0.12.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h200_sxm', backend='vllm', version='0.12.0'. Supported moe modes: ['float16', 'fp8', 'fp8_block']\n"
openai/gpt-oss-120b,GptOssForCausalLM,h200_sxm,vllm,0.12.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h200_sxm', backend='vllm', version='0.12.0'. Supported moe modes: ['float16', 'fp8', 'fp8_block']\n"
openai/gpt-oss-120b,GptOssForCausalLM,l40s,sglang,0.5.5.post3,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='l40s', backend='sglang', version='0.5.5.post3'. Supported moe modes: ['float16', 'fp8_block']\n"
openai/gpt-oss-120b,GptOssForCausalLM,l40s,sglang,0.5.5.post3,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='l40s', backend='sglang', version='0.5.5.post3'. Supported moe modes: ['float16', 'fp8_block']\n"
openai/gpt-oss-120b,GptOssForCausalLM,l40s,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='l40s', backend='trtllm', version='1.0.0'. Supported moe modes: ['float16', 'w4afp8']\n"
openai/gpt-oss-120b,GptOssForCausalLM,l40s,trtllm,1.0.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='l40s', backend='trtllm', version='1.0.0'. Supported moe modes: ['float16', 'w4afp8']\n"
openai/gpt-oss-20b,GptOssForCausalLM,a100_sxm,sglang,0.5.8,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='a100_sxm', backend='sglang', version='0.5.8'. Supported moe modes: ['float16']\n"
openai/gpt-oss-20b,GptOssForCausalLM,a100_sxm,sglang,0.5.8,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='a100_sxm', backend='sglang', version='0.5.8'. Supported moe modes: ['float16']\n"
openai/gpt-oss-20b,GptOssForCausalLM,a100_sxm,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='a100_sxm', backend='trtllm', version='1.0.0'. Supported moe modes: ['float16']\n"
openai/gpt-oss-20b,GptOssForCausalLM,a100_sxm,trtllm,1.0.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='a100_sxm', backend='trtllm', version='1.0.0'. Supported moe modes: ['float16']\n"
openai/gpt-oss-20b,GptOssForCausalLM,a100_sxm,vllm,0.12.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='a100_sxm', backend='vllm', version='0.12.0'. Supported moe modes: ['float16']\n"
openai/gpt-oss-20b,GptOssForCausalLM,a100_sxm,vllm,0.12.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='a100_sxm', backend='vllm', version='0.12.0'. Supported moe modes: ['float16']\n"
openai/gpt-oss-20b,GptOssForCausalLM,b200_sxm,sglang,0.5.6.post2,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='b200_sxm', backend='sglang', version='0.5.6.post2'. Supported moe modes: ['float16', 'fp8_block', 'nvfp4']\n"
openai/gpt-oss-20b,GptOssForCausalLM,b200_sxm,sglang,0.5.6.post2,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='b200_sxm', backend='sglang', version='0.5.6.post2'. Supported moe modes: ['float16', 'fp8_block', 'nvfp4']\n"
openai/gpt-oss-20b,GptOssForCausalLM,b200_sxm,trtllm,1.0.0rc6,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='b200_sxm', backend='trtllm', version='1.0.0rc6'. Supported moe modes: ['float16', 'fp8', 'nvfp4']\n"
openai/gpt-oss-20b,GptOssForCausalLM,b200_sxm,trtllm,1.0.0rc6,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='b200_sxm', backend='trtllm', version='1.0.0rc6'. Supported moe modes: ['float16', 'fp8', 'nvfp4']\n"
openai/gpt-oss-20b,GptOssForCausalLM,b200_sxm,trtllm,1.2.0rc5,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='b200_sxm', backend='trtllm', version='1.2.0rc5'. Supported moe modes: ['float16', 'fp8', 'nvfp4']\n"
openai/gpt-oss-20b,GptOssForCausalLM,b200_sxm,trtllm,1.2.0rc5,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='b200_sxm', backend='trtllm', version='1.2.0rc5'. Supported moe modes: ['float16', 'fp8', 'nvfp4']\n"
openai/gpt-oss-20b,GptOssForCausalLM,gb200,trtllm,1.0.0rc6,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='gb200', backend='trtllm', version='1.0.0rc6'. Supported moe modes: ['float16', 'fp8', 'nvfp4']\n"
openai/gpt-oss-20b,GptOssForCausalLM,gb200,trtllm,1.0.0rc6,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='gb200', backend='trtllm', version='1.0.0rc6'. Supported moe modes: ['float16', 'fp8', 'nvfp4']\n"
openai/gpt-oss-20b,GptOssForCausalLM,gb200,trtllm,1.2.0rc5,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='gb200', backend='trtllm', version='1.2.0rc5'. Supported moe modes: ['float16', 'fp8', 'nvfp4']\n"
openai/gpt-oss-20b,GptOssForCausalLM,gb200,trtllm,1.2.0rc5,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='gb200', backend='trtllm', version='1.2.0rc5'. Supported moe modes: ['float16', 'fp8', 'nvfp4']\n"
openai/gpt-oss-20b,GptOssForCausalLM,h100_sxm,sglang,0.5.6.post2,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h100_sxm', backend='sglang', version='0.5.6.post2'. Supported moe modes: ['float16', 'fp8_block']\n"
openai/gpt-oss-20b,GptOssForCausalLM,h100_sxm,sglang,0.5.6.post2,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h100_sxm', backend='sglang', version='0.5.6.post2'. Supported moe modes: ['float16', 'fp8_block']\n"
openai/gpt-oss-20b,GptOssForCausalLM,h100_sxm,trtllm,1.0.0rc3,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h100_sxm', backend='trtllm', version='1.0.0rc3'. Supported moe modes: ['float16', 'fp8', 'fp8_block', 'w4afp8']\n"
openai/gpt-oss-20b,GptOssForCausalLM,h100_sxm,trtllm,1.0.0rc3,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h100_sxm', backend='trtllm', version='1.0.0rc3'. Supported moe modes: ['float16', 'fp8', 'fp8_block', 'w4afp8']\n"
openai/gpt-oss-20b,GptOssForCausalLM,h100_sxm,trtllm,1.2.0rc5,agg,PASS,
openai/gpt-oss-20b,GptOssForCausalLM,h100_sxm,trtllm,1.2.0rc5,disagg,PASS,
openai/gpt-oss-20b,GptOssForCausalLM,h100_sxm,vllm,0.12.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h100_sxm', backend='vllm', version='0.12.0'. Supported moe modes: ['float16', 'fp8', 'fp8_block']\n"
openai/gpt-oss-20b,GptOssForCausalLM,h100_sxm,vllm,0.12.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h100_sxm', backend='vllm', version='0.12.0'. Supported moe modes: ['float16', 'fp8', 'fp8_block']\n"
openai/gpt-oss-20b,GptOssForCausalLM,h100_sxm,vllm,0.14.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h100_sxm', backend='vllm', version='0.14.0'. Supported moe modes: ['float16', 'fp8', 'fp8_block']\n"
openai/gpt-oss-20b,GptOssForCausalLM,h100_sxm,vllm,0.14.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h100_sxm', backend='vllm', version='0.14.0'. Supported moe modes: ['float16', 'fp8', 'fp8_block']\n"
openai/gpt-oss-20b,GptOssForCausalLM,h200_sxm,sglang,0.5.6.post2,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h200_sxm', backend='sglang', version='0.5.6.post2'. Supported moe modes: ['float16', 'fp8_block']\n"
openai/gpt-oss-20b,GptOssForCausalLM,h200_sxm,sglang,0.5.6.post2,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h200_sxm', backend='sglang', version='0.5.6.post2'. Supported moe modes: ['float16', 'fp8_block']\n"
openai/gpt-oss-20b,GptOssForCausalLM,h200_sxm,trtllm,1.0.0rc3,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h200_sxm', backend='trtllm', version='1.0.0rc3'. Supported moe modes: ['float16', 'fp8', 'fp8_block', 'w4afp8']\n"
openai/gpt-oss-20b,GptOssForCausalLM,h200_sxm,trtllm,1.0.0rc3,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h200_sxm', backend='trtllm', version='1.0.0rc3'. Supported moe modes: ['float16', 'fp8', 'fp8_block', 'w4afp8']\n"
openai/gpt-oss-20b,GptOssForCausalLM,h200_sxm,trtllm,1.2.0rc5,agg,PASS,
openai/gpt-oss-20b,GptOssForCausalLM,h200_sxm,trtllm,1.2.0rc5,disagg,PASS,
openai/gpt-oss-20b,GptOssForCausalLM,h200_sxm,vllm,0.12.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h200_sxm', backend='vllm', version='0.12.0'. Supported moe modes: ['float16', 'fp8', 'fp8_block']\n"
openai/gpt-oss-20b,GptOssForCausalLM,h200_sxm,vllm,0.12.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='h200_sxm', backend='vllm', version='0.12.0'. Supported moe modes: ['float16', 'fp8', 'fp8_block']\n"
openai/gpt-oss-20b,GptOssForCausalLM,l40s,sglang,0.5.5.post3,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='l40s', backend='sglang', version='0.5.5.post3'. Supported moe modes: ['float16', 'fp8_block']\n"
openai/gpt-oss-20b,GptOssForCausalLM,l40s,sglang,0.5.5.post3,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='l40s', backend='sglang', version='0.5.5.post3'. Supported moe modes: ['float16', 'fp8_block']\n"
openai/gpt-oss-20b,GptOssForCausalLM,l40s,trtllm,1.0.0,agg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 938, in validate\n    _validate_worker_config(self.config.worker_config, validate_context=True, validate_generation=True)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='l40s', backend='trtllm', version='1.0.0'. Supported moe modes: ['float16', 'w4afp8']\n"
openai/gpt-oss-20b,GptOssForCausalLM,l40s,trtllm,1.0.0,disagg,FAIL,"Traceback (most recent call last):\n  File ""tools/support_matrix/support_matrix.py"", line 131, in run_single_test\n    task_config = TaskConfig(**task_config_kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""src/aiconfigurator/sdk/task.py"", line 865, in __init__\n    self.validate()\n  File ""src/aiconfigurator/sdk/task.py"", line 940, in validate\n    _validate_worker_config(self.config.prefill_worker_config, validate_context=True, validate_generation=False)\n  File ""src/aiconfigurator/sdk/task.py"", line 928, in _validate_worker_config\n    _supported_or_raise(""moe"", moe_mode)\n  File ""src/aiconfigurator/sdk/task.py"", line 891, in _supported_or_raise\n    raise ValueError(\nValueError: Unsupported moe quant mode 'w4a16_mxfp4' for system='l40s', backend='trtllm', version='1.0.0'. Supported moe modes: ['float16', 'w4afp8']\n"
