# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

# ============================= User config section begin =============================
# select a model from the following list:
# GPT_7B, GPT_13B, GPT_30B, GPT_66B, GPT_175B
# LLAMA2_7B, LLAMA2_13B, LLAMA2_70B, LLAMA3.1_8B, LLAMA3.1_70B, LLAMA3.1_405B
# MOE_Mixtral8x7B, MOE_Mixtral8x22B
# DEEPSEEK_V3
# QWEN2.5_1.5B, QWEN2.5_7B, QWEN2.5_32B, QWEN2.5_72B, QWEN3_32B, QWEN3_235B

# exp1, agg
exp1:
  serving_mode: "agg"
  model_name: "DEEPSEEK_V3"
  nextn: 1 # mtp 1
  nextn_accept_rates: [0.85,0,0,0,0] # each position maps to the accept rate of the ith draft token, nextn 1 will only use the first draft token accept rate.
  runtime_config:
    isl: 4000 # input sequence length
    osl: 1000 # output sequence length
    ttft: 1000.0  # Target TTFT in ms
    tpot: 40.0   # Target TPOT in ms
  worker_config:
    system_name: "h200_sxm"
    backend_name: "trtllm"
    backend_version: "0.20.0"
    gemm_quant_mode: "fp8_block" # fp8, fp8_block, float16
    moe_quant_mode: "fp8_block" # fp8, fp8_block, w4afp8, float16
    kvcache_quant_mode: "float16" # fp8, int8, float16
    fmha_quant_mode: "float16" # fp8, float16
    comm_quant_mode: "half" # half
    num_gpu_per_worker: [4, 8]
    tp_list: [1, 2, 4, 8]
    pp_list: [1]
    dp_list: [1, 2, 4, 8]
    moe_tp_list: [1]
    moe_ep_list: [1, 2, 4, 8]

# exp2, disagg
exp2:
  serving_mode: "disagg"
  model_name: "DEEPSEEK_V3"
  nextn: 1 # mtp 1
  nextn_accept_rates: [0.85,0,0,0,0] # each position maps to the accept rate of the ith draft token, nextn 1 will only use the first draft token accept rate.
  runtime_config:
    isl: 4000 # input sequence length
    osl: 1000 # output sequence length
    ttft: 1000.0  # Target TTFT in ms
    tpot: 40.0   # Target TPOT in ms
  # each prefill worker config
  prefill_worker_config:
    system_name: "h200_sxm"
    backend_name: "trtllm"
    backend_version: "0.20.0"  
    gemm_quant_mode: "fp8_block" # fp8, fp8_block, float16
    moe_quant_mode: "fp8_block" # fp8, fp8_block, w4afp8, float16
    kvcache_quant_mode: "float16" # fp8, int8, float16
    fmha_quant_mode: "float16" # fp8, float16
    comm_quant_mode: "half" # half
    num_gpu_per_worker: [4, 8]
    tp_list: [1, 2, 4, 8]
    pp_list: [1]
    dp_list: [1] # we didn't enable attn dp here. You can enable it if you want.
    moe_tp_list: [1]
    moe_ep_list: [1, 2, 4, 8]
  # each decode worker config
  decode_worker_config:
    system_name: "h200_sxm"
    backend_name: "trtllm"
    backend_version: "0.20.0"  
    gemm_quant_mode: "fp8_block" # fp8, fp8_block, float16
    moe_quant_mode: "fp8_block" # fp8, fp8_block, w4afp8, float16
    kvcache_quant_mode: "float16" # fp8, int8, float16
    fmha_quant_mode: "float16" # fp8, float16
    comm_quant_mode: "half" # half
    num_gpu_per_worker: [4, 8]
    tp_list: [1, 2, 4, 8]
    pp_list: [1]
    dp_list: [1, 2, 4, 8]
    moe_tp_list: [1]
    moe_ep_list: [1, 2, 4, 8]
  # the whole replica config, a replica is the minimum unit of disagg deployment. It contains xPyD workers.
  # x is the number of prefill workers, y is the number of decode workers
  # then we scale replicas to meet your total gpus requirement.
  replica_config:
    num_gpu_per_replica: [8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128] # It means the searched replica will have total gpus in this list, this list will be capped by max_gpu_per_replica
    max_gpu_per_replica: 128 # max gpus per replica, if specified as 0, it means no limit. Too many gpus per replica will make the prefill/decoder worker pair complicated. no need to be too large.
    max_prefill_worker: 32 # It means in every replica, you will have up to 32 prefill workers, x_max = 32
    max_decode_worker: 32 # It means in every replica, you will have up to 32 decode workers, y_max = 32
  advanced_tuning_config:
    # advanced tuning config
    prefill_correction_scale: 0.9 # If you find the predicted prefill perf is too optimistic, you can set a scale factor to make it more realistic, throughput_corrected = throughput_predicted * prefill_correction_scale
    decode_correction_scale: 0.92 # If you find the predicted decode perf is too optimistic, you can set a scale factor to make it more realistic, throughput_corrected = throughput_predicted * decode_correction_scale
    prefill_max_batch_size: 1
    decode_max_batch_size: 512