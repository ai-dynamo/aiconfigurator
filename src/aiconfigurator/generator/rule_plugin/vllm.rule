prefill max_batch_size = (max_batch_size if max_batch_size else 1)
decode max_batch_size = (max_batch_size if max_batch_size else 128)


agg_prefill_decode gpus_per_worker = (tensor_parallel_size or 1) * (pipeline_parallel_size or 1) * (data_parallel_size or 1)
agg_prefill_decode enable_expert_parallel = ((moe_expert_parallel_size or 1) > 1)

prefill max_num_tokens = (SlaConfig.isl or 0) + 1500
decode max_num_tokens = max_batch_size
agg max_num_tokens = (max_batch_size or 0) + (SlaConfig.isl or 0) + 1500
agg max_seq_len = (SlaConfig.isl or 0) + (SlaConfig.osl or 0) + 1500

agg_prefill_decode cuda_graph_batch_sizes = ([x for x in [1,2,4,8,16,32,64,128,256,512,1024] if x < max_batch_size] + [max_batch_size] if max_batch_size else [])

when (ModelConfig.prefix or 0) > 0:
    disable_prefix_cache = false
    DynConfig.enable_router = true

when (ModelConfig.nextn or 0) > 0:
    speculative_decoding_type = "mtp"
    num_nextn_predict_layers = ModelConfig.nextn

