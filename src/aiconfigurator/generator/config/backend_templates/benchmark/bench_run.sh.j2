{% set base_concurrency = [1, 2, 8, 16, 32, 64, 128] %}
{% if BenchConfig.estimated_concurrency and BenchConfig.estimated_concurrency > 0 %}
{% set eff = BenchConfig.estimated_concurrency | int %}
{% set eff_low = (BenchConfig.estimated_concurrency * 0.95) | int %}
{% set eff_high = (BenchConfig.estimated_concurrency * 1.05) | int %}
concurrency_array=({{ (base_concurrency + [eff_low, eff, eff_high]) | join(' ') }})
{% else %}
concurrency_array=({{ base_concurrency | join(' ') }})
{% endif %}

for concurrency in "${concurrency_array[@]}"; do
  echo "Run concurrency: $concurrency"
  aiperf profile \
    -m {{ (BenchConfig.model.split('/')[-1] if BenchConfig.model else '') }} \
    --endpoint-type {{ BenchConfig.endpoint_type }} \
    -u http://{{ ServiceConfig.head_node_ip }}:{{ ServiceConfig.port }} \
    --tokenizer {{ BenchConfig.tokenizer }} \
    --isl {{ BenchConfig.isl }} --isl-stddev {{ BenchConfig.isl_stddev }} \
    --osl {{ BenchConfig.osl }} --osl-stddev {{ BenchConfig.osl_stddev }} \
    --extra-inputs ignore_eos:true \
    --extra-inputs "{\"nvext\":{\"ignore_eos\":true}}" \
    --concurrency ${concurrency} \
    --num-requests $(($concurrency*50)) \
    --warmup-request-count $(($concurrency*2)) \
    --random-seed 100 \
    --ui {{ BenchConfig.ui }} \
    --streaming
done
