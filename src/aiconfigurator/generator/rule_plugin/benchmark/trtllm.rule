BenchConfig.estimated_concurrency = ((WorkerConfig.agg_workers or 0) * (params.agg.max_batch_size or 0) if DynConfig.mode == 'agg' else (WorkerConfig.decode_workers or 0) * (params.decode.max_batch_size or 0))
prefill max_batch_size = (max_batch_size if max_batch_size else 1)
agg_decode max_batch_size = (max_batch_size if max_batch_size else 128)

prefill disable_overlap_scheduler = true
decode disable_overlap_scheduler = false
agg disable_overlap_scheduler = false

prefill max_num_tokens = SlaConfig.isl + 1500
decode max_num_tokens = max_batch_size
agg max_num_tokens = max_batch_size + SlaConfig.isl + 1500
prefill_decode cache_transceiver_max_tokens_in_buffer = SlaConfig.isl + SlaConfig.osl + 1500

agg_prefill_decode cuda_graph_batch_sizes = ((range(1, max_batch_size + 1) | list) if max_batch_size else [])

# Enforce TensorRT-LLM MoE parallelism: moe_tp Ã— moe_ep = tp
when ModelConfig.is_moe and (moe_tensor_parallel_size and moe_expert_parallel_size):
    agg_prefill_decode tensor_parallel_size = moe_tensor_parallel_size * moe_expert_parallel_size

# GPUs per worker (fallback to 1 if any dimension missing)
agg_prefill_decode gpus_per_worker = (tensor_parallel_size or 1) * (pipeline_parallel_size or 1) * (data_parallel_size or 1)

agg_prefill_decode enable_attention_dp = ((data_parallel_size or 1) > 1) and ModelConfig.is_moe

when (ModelConfig.prefix or 0) > 0:
    agg_prefill_decode disable_prefix_cache = false
    DynConfig.enable_router = true


# Speculative decoding
when (ModelConfig.nextn or 0) > 0:
    agg_decode speculative_decoding_type = "MTP"
    agg_decode num_nextn_predict_layers = ModelConfig.nextn

when kv_cache_dtype in ("float16", "bfloat16"):
    kv_cache_dtype = "auto"